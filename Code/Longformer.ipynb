{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Simple Transformer.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi_ZpTQMDNDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from simpletransformers.classification import ClassificationModel\n",
        "# from simpletransformers.classification import MultiLabelClassificationModel\n",
        "import pandas as pd\n",
        "import logging\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "import ast\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX0XdLB0HtUo",
        "colab_type": "code",
        "outputId": "bd6308a3-738a-4fef-d7fe-ba5b6fdbea1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "multi_label_classification = pd.read_csv(\"/content/multilabel_classification\", usecols=[\"text\", \"labels\"])\n",
        "multi_label_classification.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The districts of Guidan-Roumji in Niger and Do...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The week 6 was marked by a sudden apparition o...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spatial analysis shows an epidemic area common...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>During week 14 it is noted that: Burkina Faso:...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Burkina Faso: Titao was the last district in a...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                             labels\n",
              "0  The districts of Guidan-Roumji in Niger and Do...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...\n",
              "1  The week 6 was marked by a sudden apparition o...  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...\n",
              "2  Spatial analysis shows an epidemic area common...  [0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...\n",
              "3  During week 14 it is noted that: Burkina Faso:...  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...\n",
              "4  Burkina Faso: Titao was the last district in a...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7WTddJYDNDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summary_state = summary_state.drop(columns=[\"State\", \"Count\"])\n",
        "# summary_state = summary_state.rename(columns={\"Summary\":\"text\", \"Label\":\"labels\"})\n",
        "# summary_state = summary_state.reset_index(drop=True)\n",
        "multi_label_classification = multi_label_classification.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCKtwpGdDNDg",
        "colab_type": "code",
        "outputId": "665b6d30-ef38-4fd1-a3ef-313cb32d3009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "separation = int(0.8 * len(multi_label_classification))\n",
        "train_data = multi_label_classification[:separation]\n",
        "test_data = multi_label_classification[separation:]\n",
        "print(len(test_data))\n",
        "train_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The districts of Guidan-Roumji in Niger and Do...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The week 6 was marked by a sudden apparition o...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Spatial analysis shows an epidemic area common...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>During week 14 it is noted that: Burkina Faso:...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Burkina Faso: Titao was the last district in a...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>123</td>\n",
              "      <td>From week 40 to 43 (corresponding to October 2...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>124</td>\n",
              "      <td>From week 44 to 47 (corresponding to November ...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>125</td>\n",
              "      <td>From week 48 to 52 (corresponding to December ...</td>\n",
              "      <td>[0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>126</td>\n",
              "      <td>The first week of the year 2017 is marked by t...</td>\n",
              "      <td>[0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>127</td>\n",
              "      <td>At week 6 of 2017, five districts reached the ...</td>\n",
              "      <td>[0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     index  ...                                             labels\n",
              "0        0  ...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...\n",
              "1        1  ...  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...\n",
              "2        2  ...  [0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...\n",
              "3        3  ...  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...\n",
              "4        4  ...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ...\n",
              "..     ...  ...                                                ...\n",
              "123    123  ...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. ...\n",
              "124    124  ...  [0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. ...\n",
              "125    125  ...  [0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. ...\n",
              "126    126  ...  [0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. ...\n",
              "127    127  ...  [0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. ...\n",
              "\n",
              "[128 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vkBXS-RILIq",
        "colab_type": "code",
        "outputId": "23d86279-f370-4d9e-fcbc-13ef33ceae5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-hs6pqpqj\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-hs6pqpqj\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (0.15.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.11.0-cp36-none-any.whl size=666257 sha256=edc32ad764ec652ec690e021da6b645bfab027cf3726ddcb488a203c12dba5bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mmfzity8/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=0e28b2aa3b7b659fbc34f94586c5f0d811614d73f86ddc2f7a1903055e41bbae\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dYHlVKPVDNDz",
        "colab_type": "code",
        "outputId": "dfbeaa89-6a6d-436d-e3a5-ad7e49e6a297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEATROmBDND1",
        "colab_type": "code",
        "outputId": "394f3a3c-8c53-4589-f3de-e2e7ea9c180e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "#note when encoded the start of every token is added with an <s> for classification purposes\n",
        "input_ids = tokenizer.encode(\"Hello, my dog is cute\")  # Batch size 1\n",
        "print(tokenizer.decode(input_ids))\n",
        "print(tokenizer.decode(input_ids[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  4 07:40:18 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    32W /  70W |   1515MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "<s> Hello, my dog is cute</s>\n",
            "<s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r7ydk0DDND4",
        "colab_type": "code",
        "outputId": "26aa5113-1662-440c-dd02-39bcd483018e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenized = multi_label_classification[\"text\"]\\\n",
        "                                    .apply(lambda x: tokenizer.encode(x, add_special_tokens_tokens=True))\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "print(max_len)\n",
        "#padded according to max_length so we can feed the model in batches - padding works as such, add 0s after the length\n",
        "#of the original sequence l such that the new length is = max_len = 2530\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-nkKjOTDND7",
        "colab_type": "code",
        "outputId": "1e79f264-8310-4aa8-ce98-0815483581db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "np.array(padded.shape)\n",
        "#Add masking so that know BERT knows where to attend to => so it can ignore padding\n",
        "#adds a 1 everwhere where the condition of padded != 0 is satisfied, in other words everywhere there isn't padding\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "#add global attention represented with 2 - for now just add 1 at the classification <s> token\n",
        "attention_mask[:, [0, -1]] = 2\n",
        "print(attention_mask)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "input_ids = torch.tensor(padded).to(device)\n",
        "attention_mask = torch.tensor(attention_mask).to(device)\n",
        "\n",
        "def create_save_feature_representation():\n",
        "  first_time = True\n",
        "  for i in range(0,len(input_ids),2):\n",
        "    with torch.no_grad():\n",
        "      predictions, hidden_states = model(input_ids[i:i+2], attention_mask=attention_mask[i:i+2])\n",
        "      scores = predictions[:,0,:].cpu().numpy()\n",
        "      if not first_time:\n",
        "        curr = np.load(\"lang_features.npy\")\n",
        "        scores = np.concatenate((curr, scores))\n",
        "      else:\n",
        "        first_time = False\n",
        "      np.save(\"lang_features.npy\",scores)\n",
        "\n",
        "# create_save_feature_representation()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f7f6021c9159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#Add masking so that know BERT knows where to attend to => so it can ignore padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#adds a 1 everwhere where the condition of padded != 0 is satisfied, in other words everywhere there isn't padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#add global attention represented with 2 - for now just add 1 at the classification <s> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'padded' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp5EXbTWtVjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = np.load(\"lang_features.npy\")\n",
        "#convert the labels into numpy arrays since they are stored as string representations\n",
        "multi_label_classification[\"labels\"] = multi_label_classification[\"labels\"].apply(lambda x: np.fromstring(x[1:-1], dtype=np.float32, sep=' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUWtG7bDND-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, test_features = features[:separation], features[separation:]\n",
        "hidden_size = features.shape[1]\n",
        "\n",
        "#convert to float datatype that can be then cast to a tensor in PyTorch\n",
        "labels = np.array([label for label in np.array(multi_label_classification[\"labels\"].values)])\n",
        "train_labels, test_labels = labels[:separation], labels[separation:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6KwwpHUDNEB",
        "colab_type": "code",
        "outputId": "f5463c83-cc30-4be4-8589-1b35a5634678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "print(train_labels.dtype)\n",
        "epochs = 50\n",
        "neural_network = nn.Sequential(\n",
        "                    nn.Linear(hidden_size, 1000),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(1000),\n",
        "                    nn.Linear(1000, 800),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(800),\n",
        "                    nn.Linear(800, 800),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(800),\n",
        "                    nn.Linear(800, 600),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(600),\n",
        "                    nn.Linear(600, 400),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(400),\n",
        "                    nn.Linear(400, 400),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(400),\n",
        "                    nn.Linear(400, 200),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(200),\n",
        "                    nn.Linear(200, 20),\n",
        "                    nn.Sigmoid(),\n",
        "                    )\n",
        "neural_network.cuda()\n",
        "dtype = torch.cuda.FloatTensor\n",
        "neural_network.type = torch.cuda.FloatTensor\n",
        "train_features, test_features = torch.tensor(train_features).to(device), torch.tensor(test_features).to(device)\n",
        "train_labels, test_labels = torch.tensor(train_labels).to(device), torch.tensor(test_labels).to(device)\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\n",
        "optimizer = optim.Adam(neural_network.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = neural_network(train_features)\n",
        "  loss = criterion(output, train_labels)\n",
        "  print(\"Epoch {0} with training loss: {1}\".format(epoch, loss))\n",
        "  loss.backward()\n",
        "  optimizer.step()    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "Epoch 0 with training loss: 0.8404632806777954\n",
            "Epoch 1 with training loss: 0.8198241591453552\n",
            "Epoch 2 with training loss: 0.8078593015670776\n",
            "Epoch 3 with training loss: 0.7995196580886841\n",
            "Epoch 4 with training loss: 0.7932328581809998\n",
            "Epoch 5 with training loss: 0.7881842255592346\n",
            "Epoch 6 with training loss: 0.7840879559516907\n",
            "Epoch 7 with training loss: 0.7806599736213684\n",
            "Epoch 8 with training loss: 0.7776054739952087\n",
            "Epoch 9 with training loss: 0.774965226650238\n",
            "Epoch 10 with training loss: 0.772618293762207\n",
            "Epoch 11 with training loss: 0.7705239057540894\n",
            "Epoch 12 with training loss: 0.768570601940155\n",
            "Epoch 13 with training loss: 0.7667591571807861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14 with training loss: 0.7651485800743103\n",
            "Epoch 15 with training loss: 0.763664186000824\n",
            "Epoch 16 with training loss: 0.76230788230896\n",
            "Epoch 17 with training loss: 0.7610608339309692\n",
            "Epoch 18 with training loss: 0.7599023580551147\n",
            "Epoch 19 with training loss: 0.7588008046150208\n",
            "Epoch 20 with training loss: 0.7577433586120605\n",
            "Epoch 21 with training loss: 0.7567439079284668\n",
            "Epoch 22 with training loss: 0.7558051347732544\n",
            "Epoch 23 with training loss: 0.7549123764038086\n",
            "Epoch 24 with training loss: 0.7540607452392578\n",
            "Epoch 25 with training loss: 0.7532213926315308\n",
            "Epoch 26 with training loss: 0.7524320483207703\n",
            "Epoch 27 with training loss: 0.751680314540863\n",
            "Epoch 28 with training loss: 0.7509732842445374\n",
            "Epoch 29 with training loss: 0.7503077983856201\n",
            "Epoch 30 with training loss: 0.7496556043624878\n",
            "Epoch 31 with training loss: 0.7490279674530029\n",
            "Epoch 32 with training loss: 0.7484250068664551\n",
            "Epoch 33 with training loss: 0.7478465437889099\n",
            "Epoch 34 with training loss: 0.7472730875015259\n",
            "Epoch 35 with training loss: 0.7467383742332458\n",
            "Epoch 36 with training loss: 0.7462201714515686\n",
            "Epoch 37 with training loss: 0.7457151412963867\n",
            "Epoch 38 with training loss: 0.7452319860458374\n",
            "Epoch 39 with training loss: 0.7447668313980103\n",
            "Epoch 40 with training loss: 0.7443104982376099\n",
            "Epoch 41 with training loss: 0.7438696622848511\n",
            "Epoch 42 with training loss: 0.7434403300285339\n",
            "Epoch 43 with training loss: 0.74302077293396\n",
            "Epoch 44 with training loss: 0.7426111698150635\n",
            "Epoch 45 with training loss: 0.742214560508728\n",
            "Epoch 46 with training loss: 0.7418279647827148\n",
            "Epoch 47 with training loss: 0.7414478063583374\n",
            "Epoch 48 with training loss: 0.7410731315612793\n",
            "Epoch 49 with training loss: 0.740705132484436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN-JtyBdvwpl",
        "colab_type": "code",
        "outputId": "65b73e47-c96c-4fdc-e7cd-743b581afbea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def convert_to_thresholds(data):\n",
        "  return data>0.5\n",
        "\n",
        "def test_model():\n",
        "  preds = neural_network(test_features).cpu().data.numpy()\n",
        "  correct = test_labels.cpu().numpy()\n",
        "  preds = convert_to_thresholds(preds)\n",
        "  print(\"F1 Score: \", f1_score(preds, correct, average=\"micro\"))\n",
        "  print(\"ROC-AUC: \", roc_auc_score(correct, preds))\n",
        "\n",
        "test_model()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.5296950240770466\n",
            "ROC-AUC:  0.5406322402000238\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}