{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Simple Transformer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi_ZpTQMDNDY",
        "colab_type": "code",
        "outputId": "2791c3a1-b4d0-443f-da9c-0e12b4565535",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from simpletransformers.classification import ClassificationModel\n",
        "# from simpletransformers.classification import MultiLabelClassificationModel\n",
        "import pandas as pd\n",
        "import logging\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "import ast\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3736ded9-8824-4625-8385-510d1c575bf5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3736ded9-8824-4625-8385-510d1c575bf5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving multilabel_classification to multilabel_classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX0XdLB0HtUo",
        "colab_type": "code",
        "outputId": "aa2fefbe-d9f9-47bd-d4d6-b933f399ca48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "multi_label_classification = pd.read_csv(\"/content/multilabel_classification\", usecols=[\"text\", \"labels\"])\n",
        "multi_label_classification.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The districts of Guidan-Roumji in Niger and Do...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The week 6 was marked by a sudden apparition o...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spatial analysis shows an epidemic area common...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>During week 14 it is noted that: Burkina Faso:...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Burkina Faso: Titao was the last district in a...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                             labels\n",
              "0  The districts of Guidan-Roumji in Niger and Do...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...\n",
              "1  The week 6 was marked by a sudden apparition o...  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...\n",
              "2  Spatial analysis shows an epidemic area common...  [0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...\n",
              "3  During week 14 it is noted that: Burkina Faso:...  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...\n",
              "4  Burkina Faso: Titao was the last district in a...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7WTddJYDNDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summary_state = summary_state.drop(columns=[\"State\", \"Count\"])\n",
        "# summary_state = summary_state.rename(columns={\"Summary\":\"text\", \"Label\":\"labels\"})\n",
        "# summary_state = summary_state.reset_index(drop=True)\n",
        "multi_label_classification = multi_label_classification.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCKtwpGdDNDg",
        "colab_type": "code",
        "outputId": "8e037e7c-761b-454a-bdc1-b6e7a296bd55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "separation = int(0.8 * len(multi_label_classification))\n",
        "train_data = multi_label_classification[:separation]\n",
        "test_data = multi_label_classification[separation:]\n",
        "print(len(test_data))\n",
        "train_data"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The districts of Guidan-Roumji in Niger and Do...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The week 6 was marked by a sudden apparition o...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Spatial analysis shows an epidemic area common...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>During week 14 it is noted that: Burkina Faso:...</td>\n",
              "      <td>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Burkina Faso: Titao was the last district in a...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>123</td>\n",
              "      <td>From week 40 to 43 (corresponding to October 2...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>124</td>\n",
              "      <td>From week 44 to 47 (corresponding to November ...</td>\n",
              "      <td>[0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>125</td>\n",
              "      <td>From week 48 to 52 (corresponding to December ...</td>\n",
              "      <td>[0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>126</td>\n",
              "      <td>The first week of the year 2017 is marked by t...</td>\n",
              "      <td>[0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>127</td>\n",
              "      <td>At week 6 of 2017, five districts reached the ...</td>\n",
              "      <td>[0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     index  ...                                             labels\n",
              "0        0  ...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. ...\n",
              "1        1  ...  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. ...\n",
              "2        2  ...  [0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. ...\n",
              "3        3  ...  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. ...\n",
              "4        4  ...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. ...\n",
              "..     ...  ...                                                ...\n",
              "123    123  ...  [0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. ...\n",
              "124    124  ...  [0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. ...\n",
              "125    125  ...  [0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. ...\n",
              "126    126  ...  [0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. ...\n",
              "127    127  ...  [0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. ...\n",
              "\n",
              "[128 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vkBXS-RILIq",
        "colab_type": "code",
        "outputId": "aee05b88-3ac5-47e1-b390-4a6ab4ce732d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-q5ekn_ge\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-q5ekn_ge\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (0.15.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.11.0-cp36-none-any.whl size=675468 sha256=aeb1cd44ef5a543c49fb98327479ebdc190afec9cd815bcc4480d0adb8b2917d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ogt9ccn_/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=865307bef273c52a47672ba5a6219adb64f02b0e32b7a45928d4033308a55a8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKt_FUNclxax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ddc236f-e0ad-47b2-9738-e4e1acbd3971"
      },
      "source": [
        "#This code does language modeling as the first initial step of fine-tuning on a cusotm WHO report of meningitis\n",
        "#This is so the the model can learn some of the domain specific knowledge necessary to understand some of the indivial countries' reports\n",
        "!python3 \"/content/run_language_modeling.py\"  \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=longformer \\\n",
        "    --model_name_or_path=\"allenai/longformer-base-4096\" \\\n",
        "    --per_device_train_batch_size=1\\\n",
        "    --do_train \\\n",
        "    --train_data_file=\"/content/finetune.txt\" \\"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-06 08:15:32.583171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "06/06/2020 08:15:34 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "06/06/2020 08:15:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "06/06/2020 08:15:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jun06_08-15-34_96f8731c512a', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n",
            "06/06/2020 08:15:34 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-base-4096/config.json from cache at /root/.cache/torch/transformers/ea6da73c9ecf0c07b29510e423dfd58f1b8b59cba3c89383189ec8b0b6d00e82.4859554090880d98c2ceab35aa57afd197958c9c3b161bbfe07aeefb84557e54\n",
            "06/06/2020 08:15:34 - INFO - transformers.configuration_utils -   Model config LongformerConfig {\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"sep_token_id\": 2,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "06/06/2020 08:15:35 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-base-4096/config.json from cache at /root/.cache/torch/transformers/ea6da73c9ecf0c07b29510e423dfd58f1b8b59cba3c89383189ec8b0b6d00e82.4859554090880d98c2ceab35aa57afd197958c9c3b161bbfe07aeefb84557e54\n",
            "06/06/2020 08:15:35 - INFO - transformers.configuration_utils -   Model config LongformerConfig {\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"sep_token_id\": 2,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "06/06/2020 08:15:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /root/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "06/06/2020 08:15:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /root/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "06/06/2020 08:15:36 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/allenai/longformer-base-4096/pytorch_model.bin from cache at /root/.cache/torch/transformers/dfc92dbbf5c555abf807425ebdb22b55de7a17e21fe1c48cbaa5764982c1d9c0.cd65234711d2e83d420aa696eb9186cdec6ab79ef8bf090b442cf249443dfa92\n",
            "06/06/2020 08:15:44 - INFO - transformers.modeling_utils -   Weights of LongformerForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "06/06/2020 08:15:44 - INFO - filelock -   Lock 140346298003408 acquired on /content/cached_lm_LongformerTokenizer_4094_finetune.txt.lock\n",
            "06/06/2020 08:15:44 - INFO - transformers.data.datasets.language_modeling -   Loading features from cached file /content/cached_lm_LongformerTokenizer_4094_finetune.txt [took 0.001 s]\n",
            "06/06/2020 08:15:44 - INFO - filelock -   Lock 140346298003408 released on /content/cached_lm_LongformerTokenizer_4094_finetune.txt.lock\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -   ***** Running training *****\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -     Num examples = 5\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -     Instantaneous batch size per device = 1\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "06/06/2020 08:15:47 - INFO - transformers.trainer -     Total optimization steps = 15\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  20% 1/5 [00:01<00:04,  1.18s/it]\u001b[A\n",
            "Iteration:  40% 2/5 [00:02<00:03,  1.13s/it]\u001b[A\n",
            "Iteration:  60% 3/5 [00:03<00:02,  1.09s/it]\u001b[A\n",
            "Iteration:  80% 4/5 [00:04<00:01,  1.07s/it]\u001b[A\n",
            "Iteration: 100% 5/5 [00:05<00:00,  1.04s/it]\n",
            "Epoch:  33% 1/3 [00:05<00:10,  5.20s/it]\n",
            "Iteration:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  20% 1/5 [00:01<00:04,  1.00s/it]\u001b[A\n",
            "Iteration:  40% 2/5 [00:02<00:03,  1.00s/it]\u001b[A\n",
            "Iteration:  60% 3/5 [00:03<00:02,  1.00s/it]\u001b[A\n",
            "Iteration:  80% 4/5 [00:04<00:01,  1.00s/it]\u001b[A\n",
            "Iteration: 100% 5/5 [00:05<00:00,  1.00s/it]\n",
            "Epoch:  67% 2/3 [00:10<00:05,  5.14s/it]\n",
            "Iteration:   0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  20% 1/5 [00:00<00:03,  1.00it/s]\u001b[A\n",
            "Iteration:  40% 2/5 [00:02<00:03,  1.00s/it]\u001b[A\n",
            "Iteration:  60% 3/5 [00:03<00:02,  1.00s/it]\u001b[A\n",
            "Iteration:  80% 4/5 [00:04<00:01,  1.00s/it]\u001b[A\n",
            "Iteration: 100% 5/5 [00:05<00:00,  1.00s/it]\n",
            "Epoch: 100% 3/3 [00:15<00:00,  5.07s/it]\n",
            "06/06/2020 08:16:02 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "06/06/2020 08:16:02 - INFO - transformers.trainer -   Saving model checkpoint to output\n",
            "06/06/2020 08:16:02 - INFO - transformers.configuration_utils -   Configuration saved in output/config.json\n",
            "06/06/2020 08:16:05 - INFO - transformers.modeling_utils -   Model weights saved in output/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qahkWrJgCmGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dYHlVKPVDNDz",
        "colab_type": "code",
        "outputId": "64162db8-58bd-41fd-96d2-091b87068605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tokenizer = LongformerTokenizer.from_pretrained('/content/output')\n",
        "model = LongformerModel.from_pretrained('/content/output')\n",
        "model.cuda()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEATROmBDND1",
        "colab_type": "code",
        "outputId": "f868be24-3553-4688-b66f-121c94306e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "#note when encoded the start of every token is added with an <s> for classification purposes\n",
        "input_ids = tokenizer.encode(\"Hello, my dog is cute\")  # Batch size 1\n",
        "print(tokenizer.decode(input_ids))\n",
        "print(tokenizer.decode(input_ids[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> Hello, my dog is cute</s>\n",
            "<s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r7ydk0DDND4",
        "colab_type": "code",
        "outputId": "7c1b1cf1-57d0-4216-cc49-63299120bf34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenized = multi_label_classification[\"text\"]\\\n",
        "                                    .apply(lambda x: tokenizer.encode(x, add_special_tokens_tokens=True))\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "print(max_len)\n",
        "#padded according to max_length so we can feed the model in batches - padding works as such, add 0s after the length\n",
        "#of the original sequence l such that the new length is = max_len = 2530\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-nkKjOTDND7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6dc7cf72-5a11-4873-ee9c-5d660ec4288b"
      },
      "source": [
        "np.array(padded.shape)\n",
        "#Add masking so that know BERT knows where to attend to => so it can ignore padding\n",
        "#adds a 1 everwhere where the condition of padded != 0 is satisfied, in other words everywhere there isn't padding\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "#add global attention represented with 2 - for now just add 1 at the classification <s> token\n",
        "attention_mask[:, [0, -1]] = 2\n",
        "print(attention_mask)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "input_ids = torch.tensor(padded).to(device)\n",
        "attention_mask = torch.tensor(attention_mask).to(device)\n",
        "\n",
        "def create_save_feature_representation():\n",
        "  first_time = True\n",
        "  for i in range(0,len(input_ids),2):\n",
        "    with torch.no_grad():\n",
        "      predictions, hidden_states = model(input_ids[i:i+2], attention_mask=attention_mask[i:i+2])\n",
        "      scores = predictions[:,0,:].cpu().numpy()\n",
        "      if not first_time:\n",
        "        curr = np.load(\"lang_features.npy\")\n",
        "        scores = np.concatenate((curr, scores))\n",
        "      else:\n",
        "        first_time = False\n",
        "      np.save(\"lang_features.npy\",scores)\n",
        "\n",
        "create_save_feature_representation()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]\n",
            " ...\n",
            " [2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp5EXbTWtVjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = np.load(\"lang_features.npy\")\n",
        "#convert the labels into numpy arrays since they are stored as string representations\n",
        "multi_label_classification[\"labels\"] = multi_label_classification[\"labels\"].apply(lambda x: np.fromstring(x[1:-1], dtype=np.float32, sep=' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUWtG7bDND-",
        "colab_type": "code",
        "outputId": "f20ba645-19b4-4748-b7a0-e190b07dda9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_features, test_features = features[:separation], features[separation:]\n",
        "hidden_size = features.shape[1]\n",
        "\n",
        "#convert to float datatype that can be then cast to a tensor in PyTorch\n",
        "labels = np.array([label for label in np.array(multi_label_classification[\"labels\"].values)])\n",
        "train_labels, test_labels = labels[:separation], labels[separation:]\n",
        "multi_label_classification.iloc[0][\"labels\"].shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6KwwpHUDNEB",
        "colab_type": "code",
        "outputId": "a4f391a6-0e2d-44c1-82a8-af079b27a5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "print(train_labels.dtype)\n",
        "epochs = 50\n",
        "neural_network = nn.Sequential(\n",
        "                    nn.Linear(hidden_size, 1000),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(1000),\n",
        "                    nn.Linear(1000, 800),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(800),\n",
        "                    nn.Linear(800, 800),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(800),\n",
        "                    nn.Linear(800, 600),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(600),\n",
        "                    nn.Linear(600, 400),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(400),\n",
        "                    nn.Linear(400, 400),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(400),\n",
        "                    nn.Linear(400, 200),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(200),\n",
        "                    nn.Linear(200, 20),\n",
        "                    nn.Sigmoid(),\n",
        "                    )\n",
        "neural_network.cuda()\n",
        "dtype = torch.cuda.FloatTensor\n",
        "neural_network.type = torch.cuda.FloatTensor\n",
        "train_features, test_features = torch.tensor(train_features).to(device), torch.tensor(test_features).to(device)\n",
        "train_labels, test_labels = torch.tensor(train_labels).to(device), torch.tensor(test_labels).to(device)\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\n",
        "optimizer = optim.Adam(neural_network.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = neural_network(train_features)\n",
        "  loss = criterion(output, train_labels)\n",
        "  print(\"Epoch {0} with training loss: {1}\".format(epoch, loss))\n",
        "  loss.backward()\n",
        "  optimizer.step()    "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float32\n",
            "Epoch 0 with training loss: 0.8406721353530884\n",
            "Epoch 1 with training loss: 0.8182004690170288\n",
            "Epoch 2 with training loss: 0.806324303150177\n",
            "Epoch 3 with training loss: 0.7982347011566162\n",
            "Epoch 4 with training loss: 0.7923925518989563\n",
            "Epoch 5 with training loss: 0.787848711013794\n",
            "Epoch 6 with training loss: 0.7841944098472595\n",
            "Epoch 7 with training loss: 0.7809910774230957\n",
            "Epoch 8 with training loss: 0.7782623171806335\n",
            "Epoch 9 with training loss: 0.7758951783180237\n",
            "Epoch 10 with training loss: 0.7737647294998169\n",
            "Epoch 11 with training loss: 0.7718138694763184\n",
            "Epoch 12 with training loss: 0.7699738144874573\n",
            "Epoch 13 with training loss: 0.7682682275772095\n",
            "Epoch 14 with training loss: 0.7667174339294434\n",
            "Epoch 15 with training loss: 0.7653347849845886\n",
            "Epoch 16 with training loss: 0.7640621662139893\n",
            "Epoch 17 with training loss: 0.7628525495529175\n",
            "Epoch 18 with training loss: 0.7617048025131226\n",
            "Epoch 19 with training loss: 0.7606009244918823\n",
            "Epoch 20 with training loss: 0.7595781087875366\n",
            "Epoch 21 with training loss: 0.758614182472229\n",
            "Epoch 22 with training loss: 0.7576924562454224\n",
            "Epoch 23 with training loss: 0.7568144202232361\n",
            "Epoch 24 with training loss: 0.7559844255447388\n",
            "Epoch 25 with training loss: 0.7552111744880676\n",
            "Epoch 26 with training loss: 0.7544623613357544\n",
            "Epoch 27 with training loss: 0.7537353038787842\n",
            "Epoch 28 with training loss: 0.7530378103256226\n",
            "Epoch 29 with training loss: 0.7523751258850098\n",
            "Epoch 30 with training loss: 0.7517316341400146\n",
            "Epoch 31 with training loss: 0.751114010810852\n",
            "Epoch 32 with training loss: 0.7505253553390503\n",
            "Epoch 33 with training loss: 0.74994295835495\n",
            "Epoch 34 with training loss: 0.7493867874145508\n",
            "Epoch 35 with training loss: 0.7488490343093872\n",
            "Epoch 36 with training loss: 0.7483305931091309\n",
            "Epoch 37 with training loss: 0.7478307485580444\n",
            "Epoch 38 with training loss: 0.747351348400116\n",
            "Epoch 39 with training loss: 0.746888279914856\n",
            "Epoch 40 with training loss: 0.7464302778244019\n",
            "Epoch 41 with training loss: 0.7459842562675476\n",
            "Epoch 42 with training loss: 0.7455499172210693\n",
            "Epoch 43 with training loss: 0.745112955570221\n",
            "Epoch 44 with training loss: 0.7446848154067993\n",
            "Epoch 45 with training loss: 0.7442704439163208\n",
            "Epoch 46 with training loss: 0.7438544034957886\n",
            "Epoch 47 with training loss: 0.7434556484222412\n",
            "Epoch 48 with training loss: 0.7430669069290161\n",
            "Epoch 49 with training loss: 0.7426843643188477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN-JtyBdvwpl",
        "colab_type": "code",
        "outputId": "77012f23-2901-43c2-a6e3-4da606810f67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "def convert_to_thresholds(data):\n",
        "  return data>0.5\n",
        "\n",
        "def test_model():\n",
        "  preds = neural_network(test_features).cpu().data.numpy()\n",
        "  correct = test_labels.cpu().numpy()\n",
        "  print(correct)\n",
        "  preds = convert_to_thresholds(preds)\n",
        "  print(\"F1 Score: \", f1_score(preds, correct, average=\"micro\"))\n",
        "  print(\"ROC-AUC: \", roc_auc_score(correct, preds))\n",
        "\n",
        "test_model()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
            " [1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
            " [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
            " [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.]\n",
            " [0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
            " [0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
            " [0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.]]\n",
            "F1 Score:  0.5329052969502407\n",
            "ROC-AUC:  0.5662542161675027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkq8OaYYBsu8",
        "colab_type": "text"
      },
      "source": [
        "##Tests on simplified 2 state binary classification problem\n",
        "**Initial results tested with 20 labels:**\n",
        "F1 Score: 0.53\n",
        "ROC-AUC: 0.51\n",
        "\n",
        "**Tested on only the most present 8 labels** - as done with the forecasting pipeline (to check if this is a signal to noise ratio)\n",
        "F1 Score: 0.558\n",
        "ROC-AUC: 0.501\n",
        "\n",
        "So the issue here isn't due to noise - the Longformer is just not able to learn a meaningful enough feature representation that can map to the labels.\n",
        "\n",
        "Let's try to finetune our Longformer model first on some domain-specific text - reports from the WHO that outline how they plan to defeat Meningitis by 2030. After finetuning on this data, let's try to regenerate our feature representations of our report text and see if that changes our results with the multilabel classifier\n",
        "\n",
        "**After training a pre-trained model to do language modeling**\n",
        "F1 Score: 0.5329\n",
        "ROC-AUC: 0.566\n",
        "\n",
        "The results did improve but not significantly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2k9sE1KB7hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As input to finetune model - need to tokenize and feed in sentences into longformer\n",
        "#In order to feed text into the longformer - need to pass in 3 things:\n",
        "#1. input_ids which are the tokenized sentences (with the correct tags e.g. <s>)\n",
        "#2. input_masks telling model where to pay attention to - i.e. where to ignore padding etc.\n",
        "#3. token type ids which tell models about relationship of different sentences (e.g. relevant to question and answering datasets) \n",
        "# since our input consists of single sentences, we don't need it however\n",
        "\n",
        "def tokenize_sentences(sentences):\n",
        "  input_ids, attention_masks = [], []\n",
        "  for sentence in sentences:\n",
        "    #encodings is a dictionary with the encoded sequence and additional information like the attention mask\n",
        "    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=4096, pad_to_max_length=True, return_attention_mask=True)\n",
        "    input_ids.append(encoded_dict[\"input_ids\"])\n",
        "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "  return input_ids, attention_masks\n",
        "\n",
        "with open(\"finetune.txt\", \"r\") as f:\n",
        "  corpus = f.read().strip()\n",
        "  corpus = corpus.replace('.\"', '\".')\n",
        "  #split the corpus into an array of sentences - each sentence will be its own item in the array\n",
        "  sentences = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", corpus)\n",
        "  sep = int(0.8*len(sentences))\n",
        "  #save the last 20% of the data for test\n",
        "  train_dataset, test_dataset = sentences[:sep], sentences[sep:]\n",
        "\n",
        "input_ids, attention_masks = tokenize_sentences(train_dataset)\n",
        "print(len(input_ids)*4096)\n",
        "\n",
        "train_dataset, test_dataset = map(tokenize_sentences, train_sentences), map(tokenize_sentences, test_sentences)\n",
        "print(train_dataset)\n",
        "\n",
        "# config = LongformerConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "#note we're gonna be using the Longformer model we defined above so we don't need to define a new model here\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}