{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Simple Transformer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wi_ZpTQMDNDY",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "82d9c906-7df8-4b4b-abfc-55be813d17ba"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from simpletransformers.classification import ClassificationModel\n",
        "# from simpletransformers.classification import MultiLabelClassificationModel\n",
        "import pandas as pd\n",
        "import logging\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "import ast\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fbae278c-b98f-42dc-8537-f5ffbc35c30a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fbae278c-b98f-42dc-8537-f5ffbc35c30a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving threshold_classifier_data.csv to threshold_classifier_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EX0XdLB0HtUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7f09144c-5f88-414f-b34e-b0b2199e4003"
      },
      "source": [
        "multi_label_classification = pd.read_csv(\"/content/threshold_classifier_data.csv\", usecols=[\"text\", \"label\"])\n",
        "multi_label_classification.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The districts of Guidan-Roumji in Niger and Do...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In Burkina Faso, two districts are in alert at...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Niger: Distirit of Madarounfa is in the epide...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mali: Kadiolo district is in alert phase with...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spatial analysis shows an epidemic area common...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  The districts of Guidan-Roumji in Niger and Do...      0\n",
              "1  In Burkina Faso, two districts are in alert at...      2\n",
              "2   Niger: Distirit of Madarounfa is in the epide...      2\n",
              "3   Mali: Kadiolo district is in alert phase with...      2\n",
              "4  Spatial analysis shows an epidemic area common...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U7WTddJYDNDc",
        "colab": {}
      },
      "source": [
        "# summary_state = summary_state.drop(columns=[\"State\", \"Count\"])\n",
        "# summary_state = summary_state.rename(columns={\"Summary\":\"text\", \"Label\":\"labels\"})\n",
        "# summary_state = summary_state.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCKtwpGdDNDg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "6645eb38-3de3-4ef6-ada2-56b486b35f70"
      },
      "source": [
        "separation_val = int(0.7 * len(multi_label_classification))\n",
        "separation_test = int(0.85 * len(multi_label_classification))\n",
        "train_data = multi_label_classification[:separation_val]\n",
        "val_data = multi_label_classification[separation_val:separation_test]\n",
        "test_data = multi_label_classification[separation_test:]\n",
        "print(len(test_data))\n",
        "train_data"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The districts of Guidan-Roumji in Niger and Do...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In Burkina Faso, two districts are in alert at...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Niger: Distirit of Madarounfa is in the epide...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mali: Kadiolo district is in alert phase with...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spatial analysis shows an epidemic area common...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>7 The data of Nigeria that is currently in epi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>Chad: The district of Goundi reached the aler...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>Burkina Faso: The district of Kombissiri reac...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>Ethiopia: 5 Woredas reached the alert thresho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Kenya: The Suna West Sub County reached the a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>197 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  label\n",
              "0    The districts of Guidan-Roumji in Niger and Do...      0\n",
              "1    In Burkina Faso, two districts are in alert at...      2\n",
              "2     Niger: Distirit of Madarounfa is in the epide...      2\n",
              "3     Mali: Kadiolo district is in alert phase with...      2\n",
              "4    Spatial analysis shows an epidemic area common...      0\n",
              "..                                                 ...    ...\n",
              "192  7 The data of Nigeria that is currently in epi...      1\n",
              "193   Chad: The district of Goundi reached the aler...      1\n",
              "194   Burkina Faso: The district of Kombissiri reac...      2\n",
              "195   Ethiopia: 5 Woredas reached the alert thresho...      0\n",
              "196   Kenya: The Suna West Sub County reached the a...      1\n",
              "\n",
              "[197 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9vkBXS-RILIq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "fe9986d1-5018-4628-ec5f-4622b7f00c42"
      },
      "source": [
        "pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-gf6fzi7q\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-gf6fzi7q\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (1.18.5)\n",
            "Collecting tokenizers==0.8.0-rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/65/e4bf0ddb9b4d0865f1ae579132c243d2f8dd6555d9402e424434c728c8a8/tokenizers-0.8.0rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 32.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2020.4.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (0.15.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.11.0-cp36-none-any.whl size=702081 sha256=bb2eff2128f7ec698a8ac2ef1ce3e27d46ed6ca9acdd152ad2756a26620190ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ouiqjd_p/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=7edfcffa1692341c69acd32a2b7826d375910de25dd5be96ed4cd8f3ac247625\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc1 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dKt_FUNclxax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ddc236f-e0ad-47b2-9738-e4e1acbd3971"
      },
      "source": [
        "#This code does language modeling as the first initial step of fine-tuning on a cusotm WHO report of meningitis\n",
        "#This is so the the model can learn some of the domain specific knowledge necessary to understand some of the indivial countries' reports\n",
        "!python3 \"run_language_modeling.py\"  \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=longformer \\\n",
        "    --model_name_or_path=\"allenai/longformer-base-4096\" \\\n",
        "    --per_device_train_batch_size=1\\\n",
        "    --do_train \\\n",
        "    --train_data_file=\"finetune.txt\" \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: run_language_modeling.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\r\n",
            "                                [--model_type MODEL_TYPE]\r\n",
            "                                [--config_name CONFIG_NAME]\r\n",
            "                                [--tokenizer_name TOKENIZER_NAME]\r\n",
            "                                [--cache_dir CACHE_DIR]\r\n",
            "                                [--train_data_file TRAIN_DATA_FILE]\r\n",
            "                                [--eval_data_file EVAL_DATA_FILE]\r\n",
            "                                [--line_by_line] [--mlm]\r\n",
            "                                [--mlm_probability MLM_PROBABILITY]\r\n",
            "                                [--block_size BLOCK_SIZE] [--overwrite_cache]\r\n",
            "                                --output_dir OUTPUT_DIR\r\n",
            "                                [--overwrite_output_dir] [--do_train]\r\n",
            "                                [--do_eval] [--do_predict]\r\n",
            "                                [--evaluate_during_training]\r\n",
            "                                [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\r\n",
            "                                [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\r\n",
            "                                [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\r\n",
            "                                [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\r\n",
            "                                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\r\n",
            "                                [--learning_rate LEARNING_RATE]\r\n",
            "                                [--weight_decay WEIGHT_DECAY]\r\n",
            "                                [--adam_epsilon ADAM_EPSILON]\r\n",
            "                                [--max_grad_norm MAX_GRAD_NORM]\r\n",
            "                                [--num_train_epochs NUM_TRAIN_EPOCHS]\r\n",
            "                                [--max_steps MAX_STEPS]\r\n",
            "                                [--warmup_steps WARMUP_STEPS]\r\n",
            "                                [--logging_dir LOGGING_DIR]\r\n",
            "                                [--logging_first_step]\r\n",
            "                                [--logging_steps LOGGING_STEPS]\r\n",
            "                                [--save_steps SAVE_STEPS]\r\n",
            "                                [--save_total_limit SAVE_TOTAL_LIMIT]\r\n",
            "                                [--no_cuda] [--seed SEED] [--fp16]\r\n",
            "                                [--fp16_opt_level FP16_OPT_LEVEL]\r\n",
            "                                [--local_rank LOCAL_RANK]\r\n",
            "                                [--tpu_num_cores TPU_NUM_CORES]\r\n",
            "                                [--tpu_metrics_debug]\r\n",
            "run_language_modeling.py: error: argument --no_cuda: ignored explicit argument 'True'\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qahkWrJgCmGn",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYHlVKPVDNDz",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "926e204c-41b6-49bf-f7eb-1cc232f3611e"
      },
      "source": [
        "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "model.cuda()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): LongformerSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IEATROmBDND1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "20448dd4-d0d5-4f36-b195-768a3ae5fa0d"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "#note when encoded the start of every token is added with an <s> for classification purposes\n",
        "input_ids = tokenizer.encode(\"Hello, my dog is cute\")  # Batch size 1\n",
        "print(tokenizer.decode(input_ids))\n",
        "print(tokenizer.decode(input_ids[0]))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> Hello, my dog is cute</s>\n",
            "<s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4r7ydk0DDND4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f12f8d0-787e-4c33-80d6-a8a33d5e0d65"
      },
      "source": [
        "tokenized = multi_label_classification[\"text\"]\\\n",
        "                                    .apply(lambda x: tokenizer.encode(x, add_special_tokens_tokens=True))\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "print(max_len)\n",
        "#padded according to max_length so we can feed the model in batches - padding works as such, add 0s after the length\n",
        "#of the original sequence l such that the new length is = max_len = 2530\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-nkKjOTDND7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "93082dcb-379c-45ab-a0ac-480da22902b2"
      },
      "source": [
        "np.array(padded.shape)\n",
        "#Add masking so that know BERT knows where to attend to => so it can ignore padding\n",
        "#adds a 1 everwhere where the condition of padded != 0 is satisfied, in other words everywhere there isn't padding\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "#add global attention represented with 2 - for now just add 1 at the classification <s> token\n",
        "attention_mask[:, [0, -1]] = 2\n",
        "print(attention_mask)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "input_ids = torch.tensor(padded).to(device)\n",
        "attention_mask = torch.tensor(attention_mask).to(device)\n",
        "\n",
        "def create_save_feature_representation():\n",
        "  first_time = True\n",
        "  for i in range(0,len(input_ids),2):\n",
        "    with torch.no_grad():\n",
        "      predictions, hidden_states = model(input_ids[i:i+2], attention_mask=attention_mask[i:i+2])\n",
        "      scores = predictions[:,0,:].cpu().numpy()\n",
        "      if not first_time:\n",
        "        curr = np.load(\"lang_features.npy\")\n",
        "        scores = np.concatenate((curr, scores))\n",
        "      else:\n",
        "        first_time = False\n",
        "      np.save(\"lang_features.npy\",scores)\n",
        "\n",
        "create_save_feature_representation()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]\n",
            " ...\n",
            " [2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]\n",
            " [2 1 1 ... 0 0 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fp5EXbTWtVjV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abab41b8-d48b-4a73-bd48-71893acd753a"
      },
      "source": [
        "features = np.load(\"lang_features.npy\")\n",
        "print(multi_label_classification.iloc[0][\"label\"])\n",
        "#convert the labels into numpy arrays since they are stored as string representations\n",
        "# multi_label_classification[\"label\"] = multi_label_classification[\"label\"].apply(lambda x: np.fromstring(x[1:-1], dtype=np.float32, sep=','))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JzUWtG7bDND-",
        "colab": {}
      },
      "source": [
        "train_features, val_features, test_features = features[:separation_val], features[separation_val:separation_test], features[separation_test:]\n",
        "hidden_size = features.shape[1]\n",
        "\n",
        "#convert to float datatype that can be then cast to a tensor in PyTorch\n",
        "labels = np.array([label for label in np.array(multi_label_classification[\"label\"].values)])\n",
        "train_labels, val_labels, test_labels = labels[:separation_val], labels[separation_val:separation_test], labels[separation_test:]\n",
        "multi_label_classification.iloc[0][\"label\"].shape\n",
        "\n",
        "train_features, val_features, test_features = torch.tensor(train_features).to(device).float(), torch.tensor(val_features).to(device).float(),  torch.tensor(test_features).to(device).float()\n",
        "train_labels, val_labels, test_labels = torch.tensor(train_labels).to(device).long(), torch.tensor(val_labels).to(device).long(), torch.tensor(test_labels).to(device).long()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6KwwpHUDNEB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "8dc05415-9c67-4078-fd7b-25af38146f54"
      },
      "source": [
        "\n",
        "epochs = 50\n",
        "neural_network = nn.Sequential(\n",
        "                    nn.Linear(hidden_size, 1000),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(1000),\n",
        "                    nn.Linear(1000, 800),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(800),\n",
        "                    nn.Linear(800, 800),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(800),\n",
        "                    nn.Linear(800, 600),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(600),\n",
        "                    nn.Linear(600, 400),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(400),\n",
        "                    nn.Linear(400, 400),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(400),\n",
        "                    nn.Linear(400, 200),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(200),\n",
        "                    nn.Linear(200, 3),\n",
        "                    nn.Softmax(),\n",
        "                    )\n",
        "neural_network.cuda()\n",
        "dtype = torch.cuda.FloatTensor\n",
        "neural_network.type = dtype\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(neural_network.parameters(), lr=0.47e-4)\n",
        "\n",
        "print(train_features.dtype)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = neural_network(train_features)\n",
        "  loss = criterion(output, train_labels)\n",
        "  print(\"Epoch {0} with training loss: {1} val accuracy: {2}\".format(epoch, loss, test_model(val_features, val_labels)))\n",
        "  loss.backward()\n",
        "  optimizer.step()    "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "Epoch 0 with training loss: 1.1328552961349487 val accuracy: 0.4523809523809524\n",
            "Epoch 1 with training loss: 0.9776414632797241 val accuracy: 0.4523809523809524\n",
            "Epoch 2 with training loss: 0.8672659397125244 val accuracy: 0.47619047619047616\n",
            "Epoch 3 with training loss: 0.7900079488754272 val accuracy: 0.42857142857142855\n",
            "Epoch 4 with training loss: 0.7341287136077881 val accuracy: 0.42857142857142855\n",
            "Epoch 5 with training loss: 0.6941674947738647 val accuracy: 0.42857142857142855\n",
            "Epoch 6 with training loss: 0.6660510897636414 val accuracy: 0.42857142857142855\n",
            "Epoch 7 with training loss: 0.6462557911872864 val accuracy: 0.42857142857142855\n",
            "Epoch 8 with training loss: 0.6318109631538391 val accuracy: 0.40476190476190477\n",
            "Epoch 9 with training loss: 0.6210203766822815 val accuracy: 0.40476190476190477\n",
            "Epoch 10 with training loss: 0.6126052737236023 val accuracy: 0.42857142857142855\n",
            "Epoch 11 with training loss: 0.6059209108352661 val accuracy: 0.42857142857142855\n",
            "Epoch 12 with training loss: 0.6004789471626282 val accuracy: 0.42857142857142855\n",
            "Epoch 13 with training loss: 0.59598308801651 val accuracy: 0.4523809523809524\n",
            "Epoch 14 with training loss: 0.5922426581382751 val accuracy: 0.4523809523809524\n",
            "Epoch 15 with training loss: 0.5890974402427673 val accuracy: 0.42857142857142855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16 with training loss: 0.5864028334617615 val accuracy: 0.42857142857142855\n",
            "Epoch 17 with training loss: 0.5840505361557007 val accuracy: 0.42857142857142855\n",
            "Epoch 18 with training loss: 0.5820032954216003 val accuracy: 0.42857142857142855\n",
            "Epoch 19 with training loss: 0.5801969766616821 val accuracy: 0.42857142857142855\n",
            "Epoch 20 with training loss: 0.5786017775535583 val accuracy: 0.42857142857142855\n",
            "Epoch 21 with training loss: 0.5771791934967041 val accuracy: 0.42857142857142855\n",
            "Epoch 22 with training loss: 0.575903058052063 val accuracy: 0.42857142857142855\n",
            "Epoch 23 with training loss: 0.5747575759887695 val accuracy: 0.42857142857142855\n",
            "Epoch 24 with training loss: 0.5737289786338806 val accuracy: 0.42857142857142855\n",
            "Epoch 25 with training loss: 0.5728033781051636 val accuracy: 0.42857142857142855\n",
            "Epoch 26 with training loss: 0.5719597339630127 val accuracy: 0.42857142857142855\n",
            "Epoch 27 with training loss: 0.5711883902549744 val accuracy: 0.42857142857142855\n",
            "Epoch 28 with training loss: 0.5704814195632935 val accuracy: 0.42857142857142855\n",
            "Epoch 29 with training loss: 0.5698329210281372 val accuracy: 0.42857142857142855\n",
            "Epoch 30 with training loss: 0.5692371129989624 val accuracy: 0.42857142857142855\n",
            "Epoch 31 with training loss: 0.5686845183372498 val accuracy: 0.42857142857142855\n",
            "Epoch 32 with training loss: 0.5681737661361694 val accuracy: 0.42857142857142855\n",
            "Epoch 33 with training loss: 0.5677017569541931 val accuracy: 0.42857142857142855\n",
            "Epoch 34 with training loss: 0.5672637224197388 val accuracy: 0.42857142857142855\n",
            "Epoch 35 with training loss: 0.5668580532073975 val accuracy: 0.42857142857142855\n",
            "Epoch 36 with training loss: 0.566480815410614 val accuracy: 0.42857142857142855\n",
            "Epoch 37 with training loss: 0.5661296248435974 val accuracy: 0.42857142857142855\n",
            "Epoch 38 with training loss: 0.5658022165298462 val accuracy: 0.42857142857142855\n",
            "Epoch 39 with training loss: 0.5654963850975037 val accuracy: 0.42857142857142855\n",
            "Epoch 40 with training loss: 0.5652097463607788 val accuracy: 0.42857142857142855\n",
            "Epoch 41 with training loss: 0.5649402141571045 val accuracy: 0.42857142857142855\n",
            "Epoch 42 with training loss: 0.5646864771842957 val accuracy: 0.42857142857142855\n",
            "Epoch 43 with training loss: 0.5644479990005493 val accuracy: 0.42857142857142855\n",
            "Epoch 44 with training loss: 0.5642232894897461 val accuracy: 0.42857142857142855\n",
            "Epoch 45 with training loss: 0.5640110969543457 val accuracy: 0.40476190476190477\n",
            "Epoch 46 with training loss: 0.5638100504875183 val accuracy: 0.42857142857142855\n",
            "Epoch 47 with training loss: 0.5636197924613953 val accuracy: 0.42857142857142855\n",
            "Epoch 48 with training loss: 0.5634390115737915 val accuracy: 0.42857142857142855\n",
            "Epoch 49 with training loss: 0.5632672905921936 val accuracy: 0.42857142857142855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wN-JtyBdvwpl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "cf85f018-d006-45e4-ad44-d3daaba78f9b"
      },
      "source": [
        "def convert_to_thresholds(data):\n",
        "  return data>0.5\n",
        "\n",
        "def test_model(features, labels):\n",
        "  preds = neural_network(features).cpu().data.numpy()\n",
        "  correct = labels.cpu().numpy()\n",
        "  preds = np.argmax(preds, axis=1).astype(np.float32)\n",
        "  return np.sum(correct==preds)/len(labels)\n",
        "\n",
        "  # preds = convert_to_thresholds(preds)\n",
        "  # print(\"F1 Score: \", f1_score(preds, correct, average=\"micro\"))\n",
        "  # print(\"ROC-AUC: \", roc_auc_score(correct, preds))\n",
        "\n",
        "print(test_model(test_features, test_labels))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46511627906976744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qkq8OaYYBsu8"
      },
      "source": [
        "##Tests on simplified 2 state binary classification problem\n",
        "**Initial results tested with 20 labels:**\n",
        "F1 Score: 0.53\n",
        "ROC-AUC: 0.51\n",
        "\n",
        "**Tested on only the most present 8 labels** - as done with the forecasting pipeline (to check if this is a signal to noise ratio)\n",
        "F1 Score: 0.558\n",
        "ROC-AUC: 0.501\n",
        "\n",
        "So the issue here isn't due to noise - the Longformer is just not able to learn a meaningful enough feature representation that can map to the labels.\n",
        "\n",
        "Let's try to finetune our Longformer model first on some domain-specific text - reports from the WHO that outline how they plan to defeat Meningitis by 2030. After finetuning on this data, let's try to regenerate our feature representations of our report text and see if that changes our results with the multilabel classifier\n",
        "\n",
        "**After training a pre-trained model to do language modeling**\n",
        "F1 Score: 0.5329\n",
        "ROC-AUC: 0.566\n",
        "\n",
        "The results did improve but not significantly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l2k9sE1KB7hw",
        "colab": {}
      },
      "source": [
        "#As input to finetune model - need to tokenize and feed in sentences into longformer\n",
        "#In order to feed text into the longformer - need to pass in 3 things:\n",
        "#1. input_ids which are the tokenized sentences (with the correct tags e.g. <s>)\n",
        "#2. input_masks telling model where to pay attention to - i.e. where to ignore padding etc.\n",
        "#3. token type ids which tell models about relationship of different sentences (e.g. relevant to question and answering datasets) \n",
        "# since our input consists of single sentences, we don't need it however\n",
        "\n",
        "def tokenize_sentences(sentences):\n",
        "  input_ids, attention_masks = [], []\n",
        "  for sentence in sentences:\n",
        "    #encodings is a dictionary with the encoded sequence and additional information like the attention mask\n",
        "    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=4096, pad_to_max_length=True, return_attention_mask=True)\n",
        "    input_ids.append(encoded_dict[\"input_ids\"])\n",
        "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "  return input_ids, attention_masks\n",
        "\n",
        "with open(\"finetune.txt\", \"r\") as f:\n",
        "  corpus = f.read().strip()\n",
        "  corpus = corpus.replace('.\"', '\".')\n",
        "  #split the corpus into an array of sentences - each sentence will be its own item in the array\n",
        "  sentences = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", corpus)\n",
        "  sep = int(0.8*len(sentences))\n",
        "  #save the last 20% of the data for test\n",
        "  train_dataset, test_dataset = sentences[:sep], sentences[sep:]\n",
        "\n",
        "input_ids, attention_masks = tokenize_sentences(train_dataset)\n",
        "print(len(input_ids)*4096)\n",
        "\n",
        "train_dataset, test_dataset = map(tokenize_sentences, train_sentences), map(tokenize_sentences, test_sentences)\n",
        "print(train_dataset)\n",
        "\n",
        "# config = LongformerConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "#note we're gonna be using the Longformer model we defined above so we don't need to define a new model here\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}